{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Language Model\n",
    "\n",
    "Below is a diagram of the RNN computation that we will implement below. We're plugging characters into the RNN with a 1-hot encoding and expecting it to predict the next character. In this example the training data is the string \"hello\", so there are 4 letters in the vocabulary: [h,e,l,o]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rnnlm.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 4573337 characters, 67 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "# get shakespeare from http://cs.stanford.edu/people/karpathy/shakespeare.txt\n",
    "data = open('shakespeare.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has %d characters, %d unique.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "char_to_ix['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' thing when he was young,'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets sample a batch of data\n",
    "seq_length = 25 # number of characters in the batch\n",
    "p = 220000 # point in the book to sample from\n",
    "data[p:p+seq_length] # print a chunk of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 59, 27, 40, 48, 50, 15, 35, 27, 57, 48, 15, 27, 57, 15, 35, 32, 6, 15, 33, 24, 16, 48, 50, 60]\n",
      "[59, 27, 40, 48, 50, 15, 35, 27, 57, 48, 15, 27, 57, 15, 35, 32, 6, 15, 33, 24, 16, 48, 50, 60, 44]\n"
     ]
    }
   ],
   "source": [
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# lets plug the first character into the RNN\n",
    "ix_input = inputs[0]\n",
    "ix_target = targets[0]\n",
    "# encode the input character with a 1-hot representation\n",
    "x = np.zeros((vocab_size,1))\n",
    "x[ix_input] = 1\n",
    "print(x.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create random starting parameters\n",
    "hidden_size = 10\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01691059  0.01554663  0.00144864 -0.00465686  0.01337068 -0.00197203\n",
      " -0.01462834  0.0021453   0.00716481 -0.00585728]\n"
     ]
    }
   ],
   "source": [
    "# compute the hidden state\n",
    "h_prev = np.zeros((hidden_size, 1))\n",
    "h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h_prev + bh))\n",
    "print (h.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.02252254e-04, -4.09443095e-04,  2.91610545e-05,  1.17896316e-04,\n",
       "        2.01553857e-07, -7.65804840e-05, -2.04271279e-04, -1.33499420e-04,\n",
       "       -3.76494116e-04, -7.20562504e-04,  2.02576371e-04, -1.41213486e-04,\n",
       "       -6.78915199e-04,  5.91397032e-04,  1.41375269e-04,  6.63116525e-05,\n",
       "        2.54640037e-04,  1.71579179e-04, -2.75887670e-04, -7.94141193e-05,\n",
       "       -6.63724086e-04,  5.33985243e-04, -4.56874570e-04, -5.84645905e-04,\n",
       "       -3.99740338e-04, -1.84093920e-04,  3.34857922e-04, -2.02078350e-04,\n",
       "       -3.56369422e-04,  4.39302494e-04, -6.87915834e-04, -1.57101331e-04,\n",
       "       -5.85135824e-05, -2.73254714e-04,  3.95023049e-04,  1.91172537e-04,\n",
       "        1.86934623e-04,  5.44955791e-04, -3.00361815e-04,  2.30560301e-04,\n",
       "       -3.26725402e-04,  3.43734988e-04, -3.36650132e-04,  3.54415612e-04,\n",
       "       -4.64958818e-05,  5.58317235e-04,  1.51545532e-04, -4.97279834e-04,\n",
       "        1.91349645e-04, -1.79453959e-04,  4.30439637e-04,  3.97501704e-05,\n",
       "        6.01827494e-06, -6.28233124e-04, -2.48090025e-04, -1.71145760e-04,\n",
       "       -2.46412126e-04,  2.36980360e-04, -1.25024771e-04,  8.67035223e-05,\n",
       "        2.15496463e-04, -2.09981366e-04,  1.35492994e-04,  7.76797339e-06,\n",
       "       -5.52957424e-04,  3.32218531e-04,  5.70255666e-04])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the scores for next character\n",
    "y = np.dot(Why, h) + by\n",
    "y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01492899 0.01491986 0.01492641 0.01492773 0.01492598 0.01492483\n",
      " 0.01492292 0.01492398 0.01492035 0.01491522 0.014929   0.01492386\n",
      " 0.01491584 0.0149348  0.01492808 0.01492696 0.01492977 0.01492853\n",
      " 0.01492186 0.01492479 0.01491607 0.01493394 0.01491915 0.01491725\n",
      " 0.01492001 0.01492323 0.01493097 0.01492296 0.01492065 0.01493253\n",
      " 0.01491571 0.01492363 0.0149251  0.01492189 0.01493187 0.01492883\n",
      " 0.01492876 0.01493411 0.01492149 0.01492941 0.0149211  0.0149311\n",
      " 0.01492095 0.01493126 0.01492528 0.01493431 0.01492823 0.01491855\n",
      " 0.01492883 0.01492329 0.0149324  0.01492657 0.01492606 0.0149166\n",
      " 0.01492227 0.01492342 0.0149223  0.01492951 0.01492411 0.01492727\n",
      " 0.01492919 0.01492284 0.014928   0.01492609 0.01491772 0.01493093\n",
      " 0.01493449]\n",
      "probabilities sum to  1.0\n"
     ]
    }
   ],
   "source": [
    "# the scores are unnormalized log probabilities. compute the probabilities\n",
    "p = np.exp(y) / np.sum(np.exp(y))\n",
    "print(p.ravel())\n",
    "print('probabilities sum to ', p.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability assigned to the correct next character is right now:  0.01492726675670666\n"
     ]
    }
   ],
   "source": [
    "print('probability assigned to the correct next character is right now: ', p[ix_target,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cross-entropy (softmax) loss is  4.20456575473928\n"
     ]
    }
   ],
   "source": [
    "loss = -np.log(p[ix_target,0])\n",
    "print('the cross-entropy (softmax) loss is ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01492899  0.01491986  0.01492641  0.01492773  0.01492598  0.01492483\n",
      "  0.01492292  0.01492398  0.01492035  0.01491522  0.014929    0.01492386\n",
      "  0.01491584  0.0149348   0.01492808  0.01492696  0.01492977  0.01492853\n",
      "  0.01492186  0.01492479  0.01491607  0.01493394  0.01491915  0.01491725\n",
      "  0.01492001  0.01492323  0.01493097  0.01492296  0.01492065  0.01493253\n",
      "  0.01491571  0.01492363  0.0149251   0.01492189  0.01493187  0.01492883\n",
      "  0.01492876  0.01493411  0.01492149  0.01492941  0.0149211   0.0149311\n",
      "  0.01492095  0.01493126  0.01492528  0.01493431  0.01492823  0.01491855\n",
      "  0.01492883  0.01492329  0.0149324   0.01492657  0.01492606  0.0149166\n",
      "  0.01492227  0.01492342  0.0149223   0.01492951  0.01492411 -0.98507273\n",
      "  0.01492919  0.01492284  0.014928    0.01492609  0.01491772  0.01493093\n",
      "  0.01493449]\n",
      "sum of dy is  -1.0408340855860843e-17\n",
      "the gradient for the correct character (t) is: -0.9850727332432934\n",
      "the gradient for the character (a) is:  0.014925099219633975\n"
     ]
    }
   ],
   "source": [
    "# compute the gradient on y\n",
    "dy = np.copy(p)\n",
    "dy[ix_target] -= 1\n",
    "print(dy.ravel())\n",
    "print('sum of dy is ', dy.sum())\n",
    "print('the gradient for the correct character (%s) is: %s' % (ix_to_char[ix_target], dy[ix_target,0]))\n",
    "print ('the gradient for the character (a) is: ', dy[char_to_ix['a'],0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the hidden vector activations were:\n",
      "[-0.01691059  0.01554663  0.00144864 -0.00465686  0.01337068 -0.00197203\n",
      " -0.01462834  0.0021453   0.00716481 -0.00585728]\n",
      "the gradients are:\n",
      "[ 0.00559956  0.00192929  0.00399658  0.01773859  0.00816384  0.01092136\n",
      " -0.00496552 -0.01166842 -0.02226151 -0.00665568]\n",
      "the gradients dWhy have size:  (67, 10)\n",
      "a small sample is:\n",
      "[[-2.52458048e-04  2.32095461e-04  2.16267486e-05 -6.95222854e-05]\n",
      " [-2.52303667e-04  2.31953532e-04  2.16135237e-05 -6.94797720e-05]\n",
      " [-2.52414353e-04  2.32055291e-04  2.16230056e-05 -6.95102528e-05]\n",
      " [-2.52436752e-04  2.32075883e-04  2.16249244e-05 -6.95164211e-05]]\n"
     ]
    }
   ],
   "source": [
    "# we computed [y = np.dot(Why, h) + by]; Backpropagate to Why, h, and by\n",
    "dWhy = np.dot(dy, h.T)\n",
    "dh = np.dot(Why.T, dy)\n",
    "dby = np.copy(dy)\n",
    "print ('the hidden vector activations were:')\n",
    "print (h.ravel())\n",
    "print ('the gradients are:')\n",
    "print (dh.ravel())\n",
    "print ('the gradients dWhy have size: ', dWhy.shape)\n",
    "print ('a small sample is:')\n",
    "print (dWhy[:4,:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small sample of Whh:\n",
      "[[-0.0028736  -0.00195895  0.00885911 -0.00349354]\n",
      " [-0.00773272 -0.00051873  0.00219991 -0.00234756]\n",
      " [ 0.01687054 -0.01221995  0.00125455 -0.00568523]\n",
      " [-0.00031645  0.00514377  0.01194564  0.0070584 ]]\n"
     ]
    }
   ],
   "source": [
    "# we computed [h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h_prev + bh))]; \n",
    "# Backprop into Wxh, x, Whh, h_prev, bh:\n",
    "dh_before_tanh = (1-h**2)*dh\n",
    "dbh = np.copy(dh_before_tanh)\n",
    "dWxh = np.dot(dh_before_tanh, x.T)\n",
    "dWhh = np.dot(dh_before_tanh, h.T)\n",
    "dh_prev = np.dot(Whh.T, dh_before_tanh)\n",
    "print ('small sample of Whh:')\n",
    "print (Whh[:4,:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we now have the gradients for all parameters! (Wxh, Whh, Why, bh, by)\n",
    "# lets do a parameter update\n",
    "learning_rate = 0.1\n",
    "Wxh2 = Wxh - learning_rate * dWxh\n",
    "Whh2 = Whh - learning_rate * dWhh\n",
    "Why2 = Why - learning_rate * dWhy\n",
    "bh2 = bh - learning_rate * dbh\n",
    "by2 = by - learning_rate * dby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability assigned to the correct next character was:  0.01492726675670666\n",
      "probability assigned to the correct next character is now:  0.01647507098136339\n",
      "the cross-entropy (softmax) loss was  4.20456575473928\n",
      "the loss is now  4.105906890174113\n"
     ]
    }
   ],
   "source": [
    "# these parameters should be much better! lets try it out:\n",
    "h2 = np.tanh(np.dot(Wxh2, x) + np.dot(Whh2, h_prev + bh2))\n",
    "y2 = np.dot(Why2, h2) + by2\n",
    "p2 = np.exp(y2) / np.sum(np.exp(y2))\n",
    "print ('probability assigned to the correct next character was: ', p[ix_target,0])\n",
    "print ('probability assigned to the correct next character is now: ', p2[ix_target,0])\n",
    "loss2 = -np.log(p2[ix_target,0])\n",
    "print( 'the cross-entropy (softmax) loss was ', loss)\n",
    "print ('the loss is now ', loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note: the probability for the correct character went up! (and the loss went down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# putting it together with loops\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        \n",
    "    # clip to mitigate exploding gradients\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105.11784328521104\n"
     ]
    }
   ],
   "source": [
    "loss, dWxh, dWhh, dWhy, dbh, dby, hnew = lossFun(inputs, targets, h_prev)\n",
    "print (loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: write the sampling code\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is initial memory state, seed_ix is seed letter for first time step\n",
    "    n is the number of time steps to sample for\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = [] # sampled indices\n",
    "    for t in range(n):\n",
    "        pass # TODO: run the RNN for one time step, sample from distribution\n",
    "    return ixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: write the optimization loop\n",
    "# Loop over the dataset from beginning to end, sampling batches of characters seq_length long\n",
    "# Call the loss function and get the gradients\n",
    "# Perform a parameter update\n",
    "# Sample some examples from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 105.117315\n",
      "iter 100, loss: 104.983428\n",
      "iter 200, loss: 104.623506\n",
      "iter 300, loss: 104.093932\n",
      "iter 400, loss: 103.408087\n",
      "iter 500, loss: 102.742825\n",
      "iter 600, loss: 101.887765\n",
      "iter 700, loss: 101.019030\n",
      "iter 800, loss: 100.226599\n",
      "iter 900, loss: 99.278336\n",
      "iter 1000, loss: 98.311200\n",
      "iter 1100, loss: 97.269222\n",
      "iter 1200, loss: 96.135463\n",
      "iter 1300, loss: 94.971205\n",
      "iter 1400, loss: 94.009555\n",
      "iter 1500, loss: 93.148438\n",
      "iter 1600, loss: 92.125801\n",
      "iter 1700, loss: 91.396635\n",
      "iter 1800, loss: 90.822384\n",
      "iter 1900, loss: 89.993188\n",
      "iter 2000, loss: 88.971474\n",
      "iter 2100, loss: 88.298134\n",
      "iter 2200, loss: 87.808791\n",
      "iter 2300, loss: 87.190983\n",
      "iter 2400, loss: 86.830461\n",
      "iter 2500, loss: 86.598521\n",
      "iter 2600, loss: 86.067505\n",
      "iter 2700, loss: 85.818215\n",
      "iter 2800, loss: 85.895126\n",
      "iter 2900, loss: 85.631623\n",
      "iter 3000, loss: 85.174037\n",
      "iter 3100, loss: 85.296305\n",
      "iter 3200, loss: 85.270654\n",
      "iter 3300, loss: 85.047483\n",
      "iter 3400, loss: 84.906948\n",
      "iter 3500, loss: 84.596379\n",
      "iter 3600, loss: 84.436224\n",
      "iter 3700, loss: 84.399660\n",
      "iter 3800, loss: 84.315999\n",
      "iter 3900, loss: 84.127495\n",
      "iter 4000, loss: 84.059460\n",
      "iter 4100, loss: 84.000174\n",
      "iter 4200, loss: 83.676317\n",
      "iter 4300, loss: 83.834527\n",
      "iter 4400, loss: 83.535378\n",
      "iter 4500, loss: 83.242152\n",
      "iter 4600, loss: 82.924783\n",
      "iter 4700, loss: 82.976898\n",
      "iter 4800, loss: 83.061967\n",
      "iter 4900, loss: 83.094015\n",
      "iter 5000, loss: 82.961122\n",
      "iter 5100, loss: 82.787508\n",
      "iter 5200, loss: 82.812869\n",
      "iter 5300, loss: 82.654411\n",
      "iter 5400, loss: 82.512743\n",
      "iter 5500, loss: 82.472069\n",
      "iter 5600, loss: 82.329662\n",
      "iter 5700, loss: 82.101716\n",
      "iter 5800, loss: 82.110144\n",
      "iter 5900, loss: 82.089779\n",
      "iter 6000, loss: 82.000559\n",
      "iter 6100, loss: 82.109607\n",
      "iter 6200, loss: 82.205160\n",
      "iter 6300, loss: 82.141634\n",
      "iter 6400, loss: 82.378974\n",
      "iter 6500, loss: 82.279731\n",
      "iter 6600, loss: 82.348806\n",
      "iter 6700, loss: 82.716078\n",
      "iter 6800, loss: 82.783702\n",
      "iter 6900, loss: 82.997785\n",
      "iter 7000, loss: 82.991109\n",
      "iter 7100, loss: 83.114908\n",
      "iter 7200, loss: 83.141573\n",
      "iter 7300, loss: 83.277007\n",
      "iter 7400, loss: 83.253893\n",
      "iter 7500, loss: 83.365069\n",
      "iter 7600, loss: 83.318780\n",
      "iter 7700, loss: 83.197795\n",
      "iter 7800, loss: 83.067030\n",
      "iter 7900, loss: 82.758193\n",
      "iter 8000, loss: 82.675738\n",
      "iter 8100, loss: 82.525662\n",
      "iter 8200, loss: 82.599984\n",
      "iter 8300, loss: 82.616890\n",
      "iter 8400, loss: 82.558240\n",
      "iter 8500, loss: 82.561061\n",
      "iter 8600, loss: 82.582244\n",
      "iter 8700, loss: 82.594571\n",
      "iter 8800, loss: 82.484195\n",
      "iter 8900, loss: 82.871322\n",
      "iter 9000, loss: 82.777270\n",
      "iter 9100, loss: 82.931874\n",
      "iter 9200, loss: 83.166242\n",
      "iter 9300, loss: 83.114963\n",
      "iter 9400, loss: 82.804766\n",
      "iter 9500, loss: 82.629507\n",
      "iter 9600, loss: 82.648924\n",
      "iter 9700, loss: 82.562232\n",
      "iter 9800, loss: 82.470136\n",
      "iter 9900, loss: 82.096858\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent\n",
    "n, p = 0, 0\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "learning_rate = 1e-3\n",
    "while n < 10000:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby]):\n",
    "        param += -learning_rate * dparam\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aunmtinsrl:ne dew t:V m Au:rht Svonresim,sp\n",
      "e om reoYI ermew bsr yoserlgyte whet yd\n",
      "e  rtofot\n",
      "u.rlbt lilEbutSeomfna sOtimb UeN nedeye o, hlsnhAnnm ireD lt HsGkAsaIgeeoA,de a:\n",
      "icnyhnd, o i lO aI rlnJeg nhlmr-s ,ngO 'etXabnroneT tElcito\n",
      "ui pnaa ,n y tnrshs:e TdaEheot  ipuas ynon:o sslo\n",
      "e m dtnyA\n",
      "wfhksrrib\n",
      "eu\n",
      "r  t h herrecluhacFi t tlpaueem it Hsd aertn\n",
      " lie orine lod d AowredI thaaenit \n",
      "CNwso mIhizben.:dsIaum a?e:UrrP vn e ld a Sglah desh to eh \n",
      " tia ulpauie mvoiesre os \n",
      "t milhwr wt aggooali\n",
      "hTdei t&iit, ndyn mGlhLen\n",
      " ry ohdse\n",
      " o\n",
      "lw terKn mgwtwTne io eo casrdarCyh n d n wP, ee eo e r.ed nKiN?dtmda:A\n",
      "e anralrcqwant obtie g\n",
      "rdeal\n",
      "l oYn itqsl r re Csr rsto nt atnoldhO]yrdi , Ishh htgo o yOsliilmoogoeMhae lheaennh etsy n tabe yl]k ehieed;:\n",
      "sefperasagrse.rmp\n",
      "oantfrLE, unanne sseatenrs:c  wo tyh uhn te mthh  hy ileihye' e ed .ev t Bc hiA ta y und ureh res rey \n",
      "muavmno af ufeaolidAkeratisEn iMit bdg;KeltIlnd sCCtanpTH l tGakh sLraeriohradbene \n",
      "' le mit,t td sancp edyd ndI ktrnolar hrsNWe\n",
      "au  oi\n"
     ]
    }
   ],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "sample_ix = sample(hprev, char_to_ix['a'], 1000)\n",
    "txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "print (txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
